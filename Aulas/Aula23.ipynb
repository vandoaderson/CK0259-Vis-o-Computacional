{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 23 – Redes Neurais Artificiais: Perceptron Multicamadas\n",
    "\n",
    "## 1. Introdução\n",
    "\n",
    "Na aula anterior, vimos que o perceptron simples, por ser um classificador linear, tinha dificuldades em classificar muitas imagens quando o espaço das características não permitia separação por um hiperplano. A solução encontrada para aproveitar a mesma ideia do modelo perceptron para neurônio artificial foi unir um conjunto deles em diferentes camadas, onde a saída dos neurônios da camada N estão conectadas às entradas dos neurônios na camada N+1. Dessa forma, montamos uma rede neural propriamente dita.\n",
    "\n",
    "Embora ainda seja uma arquitetura de rede bem simples, com camadas bem divididas e sem recursões, ou seja, os dados fluem em apenas uma direção na rede (feedforward), conseguimos com isso resolver o problema do perceptron simples. A ideia geral é “traçar” várias retas (hiperplanos) no espaço e o resultado será dado pela união deles, ou seja, agora conseguimos traçar “curvas” para recortar o espaço. \n",
    "\n",
    "A ideia ainda é ter um classificador linear, ou seja, na última camada temos apenas um neurônio que dirá se os dados que entraram na rede pertencem ou não a uma determinada classe. Se precisarmos de mais classes, podemos treinar redes diferentes para cada uma ou multiplicar apenas a camada de saída (o que pode dificultar o trabalho do algoritmo de aprendizagem). A quantidade de neurônios por camada é um parâmetro que deve ser manualmente ajustado e depende da complexidade do problema de classificação enfrentado, sendo a camada inicial normalmente sendo apenas uma “receptora” das características (ou seja, não são neurônios propriamente ditos). Quanto às camadas, normalmente temos três: entrada, “escondida” e saída.\n",
    "\n",
    "A “mágica” do perceptron multicamadas acontece no seu algoritmo de aprendizagem. Encontrar os parâmetros corretos para cada “reta” para que o espaço seja devidamente recortado é o desafio. O algoritmo que realiza esse processo é conhecido como “backpropagation” (retropropagação). A ideia é calcular o erro do algoritmo (diferença entre resposta dada e resposta correta) e, através dele, calcular o gradiente na função de erro e, portanto, identificando a direção que leva a uma redução dele. Esse gradiente indica como devemos modificar os parâmetros para ter um erro menor. A rede neural define uma função e, portanto, matematicamente, isso poderia ser feito em um só passo. O backpropagation, entretanto, propõe como se modifica os pesos uma camada por vez, da saída até a entrada, resultando num algoritmo de implementação relativamente simples. \n",
    "\n",
    "Para mais detalhes sobre o backpropagation, como implementá-lo e como, matematicamente, ele funciona, recorra à leitura complementar. Não entraremos a fundo no seu funcionamento pois, como já dito, esse assunto é abordado mais especificamente em disciplinas de aprendizagem de máquina. Nosso propósito aqui é entendê-lo em nível suficiente para conseguir aplicá-lo adequadamente na classificação de imagens.\n",
    "\n",
    "## 2. Leitura complementar\n",
    "\n",
    "Livro E – Seção 11.5.2\n",
    "\n",
    "Livro 1 – Seção 9.4.2\n",
    "\n",
    "Livro 3 – Seção 12.2.3\n",
    "\n",
    "## 3. Exercícios\n",
    "\n",
    "Repita o exercício da aula 22 agora utilizando um perceptron multicamadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
