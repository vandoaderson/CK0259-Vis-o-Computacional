{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 24 – Validação\n",
    "\n",
    "## 1. Introdução\n",
    "\n",
    "Como vimos nas aulas anteriores, os algoritmos de visão computacional são incertos. Todos eles se baseiam em extrair características genéricas das imagens, pegar esses dados e informar uma probabilidade daqueles pertencerem a uma determinada classe de objetos. Precisamos, então, formalizar métodos de avaliação dos algoritmos para saber o quão bons são na tarefa de classificação das imagens. Como são algoritmos que envolvem um conjunto de dados que são observados numa fase de treino, a avaliação está fortemente relacionada ao treino, ou seja, não é uma informação absoluta sobre a qualidade do algoritmo, mas sim sobre como determinado algoritmo se comporta após treino com determinado conjunto de dados. Entre os métodos de validação, temos a contagem simples, conjunto de teste, matriz de confusão e validação cruzada. \n",
    "\n",
    "Na contagem simples, após treinado, entramos novamente no classificador os dados usados no treino e contamos a taxa de erro dele, ou seja, número de erros dividido pelo tamanho do conjunto. É uma abordagem ingênua, pois espera-se que, de fato, os métodos “reconheçam” pelo menos os dados que já foram observados no treino e para os quais eles se ajustaram. Mas também não é verdade que isso necessariamente vai ocorrer, dependendo do algoritmo, e, portanto, dá uma certa indicação de qualidade do classificador.\n",
    "\n",
    "Para evitar o problema do “vício” da contagem simples, o método de conjunto de teste melhora ao usar dados diferentes para teste. Para a realização de tal validação, separa-se uma parte do conjunto de dados que temos classificados e poderíamos utilizar para treino para fazer o teste. Normalmente 70% dos dados vão para o treino e 30% são utilizados apenas no teste. A taxa de erro é medida, então, no conjunto de teste. Espera-se também que haja um balanceamento nas classes, isto é, que a mesma proporção que cada classe aparece no conjunto de treino seja também no conjunto de teste.\n",
    "\n",
    "As métricas apresentadas até então se baseiam numa estimativa global do erro. Uma alternativa a isso é a matriz de confusão, que indica por classe a taxa de erro, mostrando quais as classes estão trazendo mais dificuldade para o classificador.\n",
    "\n",
    "Por fim, temos a classificação cruzada. Nesse método, dividimos o conjunto de treino em k subconjuntos de tamanhos iguais. Repetimos o treino e o teste para cada um desses subconjuntos. Ao treinar um conjunto, utilizamos os outros k-1 conjuntos restantes como testes para ele.\n",
    "\n",
    "## 2. Leitura Complementar\n",
    "\n",
    "Livro E – Seção 11.8\n",
    "\n",
    "Livro 1 – Seção 9.5\n",
    "\n",
    "## 3. Exercícios\n",
    "\n",
    "Faça a estimativa de qualidade do seu classificador MLP construído na aula 23 utilizando Validação Cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
