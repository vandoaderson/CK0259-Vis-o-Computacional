{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 22 – Redes Neurais Artificiais: Perceptron Simples\n",
    "\n",
    "## 1. Introdução\n",
    "\n",
    "Continuando com algoritmos para classificação das imagens, entramos de vez no mundo da Inteligência Artificial para conseguir realizar essa tarefa. Até agora, tínhamos algoritmos basicamente estatísticos e hoje entramos em um modelo inspirado no modo de funcionamento do cérebro, as Redes Neurais Artificiais. Mas, de fato, as técnicas de aprendizagem de máquina se aproveitam desses dois mundos.\n",
    "\n",
    "Começaremos falando do Perceptron Simples. É um modelo matemático para um neurônio artificial  que simula simplificadamente um neurônio biológico. Embora sua primeira implementação tenha se dado em software, foi concebido para ser desenvolvido em hardware, como de fato o fora em seguida, tendo sido construído uma máquina cuja função era justamente o reconhecimento de imagens, ainda no final dos anos 1950 e início dos anos 1960.\n",
    "\n",
    "O perceptron simples é um classificador linear. Ou seja, considerando o espaço 2D, só consegue dividi-lo por uma reta, ou um plano em 3D e assim por diante. Assim, é fácil encontrar problemas que um perceptron simples não resolve, como o simples operador lógico “ou exclusivo”. E, de fato, a classificação de imagens, em muitos casos, não é um problema linearmente separável. Assim, houve um desânimo inicial com o projeto e nos anos seguintes à sua criação a ideia de redes neurais acabou caindo em descrédito. Só anos depois, por volta do final dos anos 1980 é que avanços foram conseguidos e os resultados começaram a aparecer.\n",
    "\n",
    "O que quer dizer “a classificação de imagens não é um problema linearmente separável”? Bom, se considerarmos que estamos passando um vetor de características para o classificador (pode ser a própria imagem, mas, como estudamos até aqui, podemos extrair e organizar os dados que realmente importam), pode ser que imagens que deveriam aparecer em classes diferentes compartilhem algumas características que, plotando num espaço cujos eixos indicam as características, seja impossível traçar um hiperplano tal que de um lado dele fiquem todos os dados que representem imagens de um objeto A e do outro lado fiquem todos os dados de um objeto B. É importante também ressaltar que um perceptron é um classificador binário e, portanto, ele só consegue dizer se um dado pertence ou não a uma classe (se tivermos só duas classes A e B, consegue dizer se é da classe A ou da classe B).\n",
    "\n",
    "Suponha a seguinte distribuição, onde os pontos “1” seriam, por exemplo, dados que representam imagens de cachorros, enquanto os pontos “0” representam imagens de gatos. Suponha ainda que estamos pegando apenas duas características (x1 e x2) das imagens para representá-las, ou seja, temos um espaço 2D. Perceba que é impossível traçar uma reta tal que todos os gatos fiquem de um lado e todos os cachorros fiquem do outro (esse é justamente o exemplo “ou exclusivo”).\n",
    "\n",
    "![titlle](img/fig22.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "De fato, o neurônio é só um somatório de variáveis (os dados de entrada) multiplicados por pesos. Isso define justamente uma função linear (uma reta, se 2D). A classificação acontece por uma função limiar que pega o resultado do somatório, indicando valores “acima” ou “abaixo” da reta.\n",
    "\n",
    "O aprendizado do neurônio implica em movimentar essa reta pelo espaço até encontrar a posição que faz a divisão correta. Ou seja, o algoritmo de aprendizado em um perceptron ajusta os pesos do somatório e o valor de limiar (ou o coeficiente linear). Para mais detalhes sobre a matemática do neurônio e o algoritmo de aprendizagem, recorra à leitura complementar. \n",
    "\n",
    "Entretanto, mesmo sendo um classificador bem simples, é possível realizar trabalhos de classificação de imagens bem interessantes. O problema da linearidade, entretanto, se resolve quando conectamos esses neurônios de forma complexa, em rede, criando várias camadas de perceptron. Temos assim os Perceptrons Multicamadas (MLP – Multi Layer Perceptrons), assunto da próxima aula.\n",
    "\n",
    "## 2. Leitura complementar\n",
    "\n",
    "Livro E – Seção 11.5.1\n",
    "\n",
    "Livro 1 – Seção 9.4.1\n",
    "\n",
    "Livro 3 – Seção12.2.3\n",
    "\n",
    "## 3. Exercícios\n",
    "\n",
    "Repita o exercício da aula 20 agora utilizando um perceptron simples. A OpenCV traz a implementação de perceptrons, então é só treinar e utilizar para classificar novas imagens fora do conjunto de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
