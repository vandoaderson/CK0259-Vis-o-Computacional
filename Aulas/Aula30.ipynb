{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 30 – Generative Adversarial Networks\n",
    "\n",
    "## 1. Introdução\n",
    "\n",
    "As Generative Adversarial Networks (Redes adversárias gerativas) ou GANs, são a tecnologia por trás da técnica conhecida por Deep Fake, onde imagens que nunca existiram são geradas, geralmente utilizadas para trocar personagens em imagens. Não seria possível esgotar o assunto em uma aula, então o objetivo aqui é mostrar o conceito e indicar caminhos para quem quiser depois conhecer melhor por conta própria.\n",
    "\n",
    "A ideia por trás das GANs é colocar duas redes convolucionais, G (geradora) e D (discriminadora), para competirem no processo de treinamento, sendo que G gera a imagem, enquanto D classifica a imagem. O objetivo é que G engane D e que suas imagens geradas sejam classificadas como imagens de objetos reais que D é capaz de reconhecer. É importante enfatizar que ambas treinam, isto é, se D é enganada, essa informação é utilizada para melhorar seus parâmetros para que ela consiga reconhecer melhor imagens geradas, dificultando, então, cada vez mais, o trabalho de G, que precisa se desenvolver para gerar imagens cada vez melhores.\n",
    "\n",
    "O outro ponto importante aqui é como fazer uma rede convolucional gerar, e não classificar imagens. Bom, a ideia é inverter o fluxo de informações, isto é, os neurônios que seriam de “saída”, ou seja, os que classificariam, recebem a entrada, ou seja, a informação de qual objeto queremos que seja gerado. Os sinais percorrem a rede até a entrada, onde seria dada a imagem, ou seja, seus pixels, informando as supostas intensidades de pixels que deveriam ser dadas de entrada para a classificação desejada. Para ser mais preciso, isso é reconhecido como uma rede “deconvolucional”.\n",
    "\n",
    "Basicamente, a imagem gerada é uma combinação das características aprendidas para identificar uma imagem. GANs rudimentares geram resultados que enganam pessoas em uma passagem rápida pelas imagens, pois as características esperadas estão lá. Porém, muitas vezes, parecem quebra-cabeças mal montados (peças corretas, mas nos cantos errados), quando observamos melhor. Entretanto, o avanço da técnica está permitindo a geração de imagens cada vez mais difíceis de se identificar como falsas. A principal característica necessária para a geração de boas GANs é um treinamento intensivo, com uma base muito grande de exemplos, sendo computacionalmente pesado o processo de gerar, embora executar não seja tão exigente.\n",
    "\n",
    "## 2. Material Complementar\n",
    "\n",
    "http://www.iangoodfellow.com/slides/2016-12-9-gans.pdf\n",
    "\n",
    "https://www.youtube.com/watch?v=RvgYvHyT15E\n",
    "\n",
    "## 3. Exercícios\n",
    "\n",
    "- Realize o seguinte tutorial: \n",
    "https://towardsdatascience.com/build-a-super-simple-gan-in-pytorch-54ba349920e4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utils.py\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def create_binary_list_from_int(number: int) -> List[int]:\n",
    "    \"\"\"Creates a list of the binary representation of a positive integer\n",
    "    Args:\n",
    "        number: An integer\n",
    "    Returns:\n",
    "        The binary representation of the provided positive integer number as a list.\n",
    "    \"\"\"\n",
    "    if number < 0 or type(number) is not int:\n",
    "        raise ValueError(\"Only Positive integers are allowed\")\n",
    "\n",
    "    return [int(x) for x in list(bin(number))[2:]]\n",
    "\n",
    "\n",
    "def generate_even_data(\n",
    "    max_int: int, batch_size: int = 16\n",
    ") -> Tuple[List[int], List[List[int]]]:\n",
    "    \"\"\"An infinite data generator which yields\n",
    "    Args:\n",
    "        max_int: The maximum input integer value\n",
    "        batch_size: The size of the training batch.\n",
    "    Returns:\n",
    "        A Tuple with the labels and the input data.\n",
    "        labels:\n",
    "        data:\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the number of binary places needed to represent the maximum number\n",
    "    max_length = int(math.log(max_int, 2))\n",
    "\n",
    "    # Sample batch_size number of integers in range 0-max_int\n",
    "    sampled_integers = np.random.randint(0, int(max_int / 2), batch_size)\n",
    "\n",
    "    # create a list of labels all ones because all numbers are even\n",
    "    labels = [1] * batch_size\n",
    "\n",
    "    # Generate a list of binary numbers for training.\n",
    "    data = [create_binary_list_from_int(int(x * 2)) for x in sampled_integers]\n",
    "    data = [([0] * (max_length - len(x))) + x for x in data]\n",
    "\n",
    "    return labels, data\n",
    "\n",
    "\n",
    "def convert_float_matrix_to_int_list(\n",
    "    float_matrix: np.array, threshold: float = 0.5\n",
    ") -> List[int]:\n",
    "    \"\"\"Converts generated output in binary list form to a list of integers\n",
    "    Args:\n",
    "        float_matrix: A matrix of values between 0 and 1 which we want to threshold and convert to\n",
    "            integers\n",
    "        threshold: The cutoff value for 0 and 1 thresholding.\n",
    "    Returns:\n",
    "        A list of integers.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        int(\"\".join([str(int(y)) for y in x]), 2) for x in float_matrix >= threshold\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models.py\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_length: int):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dense_layer = nn.Linear(int(input_length), int(input_length))\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.dense_layer(x))\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_length: int):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense = nn.Linear(int(input_length), 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.dense(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a36e8fb1035c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-a36e8fb1035c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(max_int, batch_size, training_steps, learning_rate, print_output_every_n_steps)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# to make things the discriminator classifies as true.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mgenerator_discriminator_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mgenerator_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_discriminator_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mgenerator_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mgenerator_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2516\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2517\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2518\u001b[0;31m         raise ValueError(\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\n\u001b[0m\u001b[1;32m   2519\u001b[0m                          \"Please ensure they have the same size.\".format(target.size(), input.size()))\n\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "#Train.py\n",
    "from typing import Tuple\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#from models import Discriminator, Generator\n",
    "#from utils import generate_even_data, convert_float_matrix_to_int_list\n",
    "\n",
    "\n",
    "def train(\n",
    "    max_int: int = 128,\n",
    "    batch_size: int = 16,\n",
    "    training_steps: int = 500,\n",
    "    learning_rate: float = 0.001,\n",
    "    print_output_every_n_steps: int = 10,\n",
    ") -> Tuple[nn.Module]:\n",
    "    \"\"\"Trains the even GAN\n",
    "    Args:\n",
    "        max_int: The maximum integer our dataset goes to.  It is used to set the size of the binary\n",
    "            lists\n",
    "        batch_size: The number of examples in a training batch\n",
    "        training_steps: The number of steps to train on.\n",
    "        learning_rate: The learning rate for the generator and discriminator\n",
    "        print_output_every_n_steps: The number of training steps before we print generated output\n",
    "    Returns:\n",
    "        generator: The trained generator model\n",
    "        discriminator: The trained discriminator model\n",
    "    \"\"\"\n",
    "    input_length = int(math.log(max_int, 2))\n",
    "\n",
    "    # Models\n",
    "    generator = Generator(input_length)\n",
    "    discriminator = Discriminator(input_length)\n",
    "\n",
    "    # Optimizers\n",
    "    generator_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "    discriminator_optimizer = torch.optim.Adam(\n",
    "        discriminator.parameters(), lr=learning_rate\n",
    "    )\n",
    "\n",
    "    # loss\n",
    "    loss = nn.BCELoss()\n",
    "\n",
    "    for i in range(training_steps):\n",
    "        # zero the gradients on each iteration\n",
    "        generator_optimizer.zero_grad()\n",
    "\n",
    "        # Create noisy input for generator\n",
    "        # Need float type instead of int\n",
    "        noise = torch.randint(0, 2, size=(batch_size, input_length)).float()\n",
    "        generated_data = generator(noise)\n",
    "\n",
    "        # Generate examples of even real data\n",
    "        true_labels, true_data = generate_even_data(max_int, batch_size=batch_size)\n",
    "        true_labels = torch.tensor(true_labels).float()\n",
    "        true_data = torch.tensor(true_data).float()\n",
    "\n",
    "        # Train the generator\n",
    "        # We invert the labels here and don't train the discriminator because we want the generator\n",
    "        # to make things the discriminator classifies as true.\n",
    "        generator_discriminator_out = discriminator(generated_data)\n",
    "        generator_loss = loss(generator_discriminator_out, true_labels)\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # Train the discriminator on the true/generated data\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        true_discriminator_out = discriminator(true_data)\n",
    "        true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
    "\n",
    "        # add .detach() here think about this\n",
    "        generator_discriminator_out = discriminator(generated_data.detach())\n",
    "        generator_discriminator_loss = loss(\n",
    "            generator_discriminator_out, torch.zeros(batch_size)\n",
    "        )\n",
    "        discriminator_loss = (\n",
    "            true_discriminator_loss + generator_discriminator_loss\n",
    "        ) / 2\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "        if i % print_output_every_n_steps == 0:\n",
    "            print(convert_float_matrix_to_int_list(generated_data))\n",
    "\n",
    "    return generator, discriminator\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: /home/vando/ (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute '/home/vando/'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.007s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vando/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#TrainTest.py\n",
    "import math\n",
    "from typing import List\n",
    "import unittest\n",
    "\n",
    "from ddt import data, ddt, unpack\n",
    "import torch\n",
    "\n",
    "#from utils import convert_float_matrix_to_int_list\n",
    "#from train import train\n",
    "\n",
    "\n",
    "@ddt\n",
    "class TrainTest(unittest.TestCase):\n",
    "    @data(\n",
    "        (128, 16, 500, 0.001, \"Test reasonable parameters\"),\n",
    "        (256, 16, 500, 0.001, \"Test reasonable parameters\"),\n",
    "    )\n",
    "    @unpack\n",
    "    def test_train(\n",
    "        self,\n",
    "        max_int: int,\n",
    "        batch_size: int,\n",
    "        training_steps: int,\n",
    "        learning_rate: float,\n",
    "        test_description: str,\n",
    "    ):\n",
    "        input_length = int(math.log(max_int, 2))\n",
    "        generator, discriminator = train(\n",
    "            max_int=max_int,\n",
    "            batch_size=batch_size,\n",
    "            training_steps=training_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            print_output_every_n_steps=1000000,\n",
    "        )\n",
    "        noise = torch.randint(0, 2, size=(batch_size, input_length)).float()\n",
    "        generated_data = generator(noise)\n",
    "        for num in convert_float_matrix_to_int_list(generated_data):\n",
    "            self.assertEqual(num % 2, 0, test_description)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: /home/vando/ (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute '/home/vando/'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vando/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#UtilTest.py\n",
    "import math\n",
    "from typing import List\n",
    "import unittest\n",
    "\n",
    "from ddt import data, ddt, unpack\n",
    "import numpy as np\n",
    "\n",
    "# from utils import (\n",
    "#     create_binary_list_from_int,\n",
    "#     generate_even_data,\n",
    "#     convert_float_matrix_to_int_list,\n",
    "# )\n",
    "\n",
    "\n",
    "@ddt\n",
    "class UtilsTest(unittest.TestCase):\n",
    "    @data(\n",
    "        (1, [1], \"test single odd\"),\n",
    "        (0, [0], \"test zero\"),\n",
    "        (7, [1, 1, 1], \"test small prime\"),\n",
    "        (128, [1, 0, 0, 0, 0, 0, 0, 0], \"test large even\"),\n",
    "        (129, [1, 0, 0, 0, 0, 0, 0, 1], \"test large odd\"),\n",
    "    )\n",
    "    @unpack\n",
    "    def test_create_binary_list_from_int(\n",
    "        self, number: int, expected: List[int], test_description: str\n",
    "    ):\n",
    "        output = create_binary_list_from_int(number)\n",
    "        self.assertListEqual(expected, output, test_description)\n",
    "\n",
    "    @data(\n",
    "        (2, 2, \"test small input\"),\n",
    "        (128, 2, \"test standard input\"),\n",
    "        (128, 16, \"test standard input big batch size\"),\n",
    "        (1024, 16, \"test big input\"),\n",
    "    )\n",
    "    @unpack\n",
    "    def test_generate_even_data(\n",
    "        self, max_int: int, batch_size: int, test_description: str\n",
    "    ):\n",
    "        labels, output = generate_even_data(max_int, batch_size=batch_size)\n",
    "        self.assertEqual(len(labels), batch_size)\n",
    "        self.assertEqual(len(output), batch_size)\n",
    "        for binary_num in output:\n",
    "            self.assertEqual(binary_num[-1], 0, test_description)\n",
    "            self.assertEqual(len(binary_num), math.log(max_int, 2), test_description)\n",
    "\n",
    "    @data(([[0.6, 0.2], [0.5, 0.5]], [2, 3], \"test two values\"))\n",
    "    @unpack\n",
    "    def test_convert_float_matrix_to_int_list(\n",
    "        self, matrix: List[List[float]], expected: List[int], test_description: str\n",
    "    ):\n",
    "        matrix = np.array(matrix)\n",
    "        output = convert_float_matrix_to_int_list(np.array(matrix))\n",
    "        self.assertListEqual(expected, output, test_description)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
